# Playing repeated games with Large Language Models

Data and code to reproduce our analyses. Preprint available [here](https://arxiv.org/abs/2305.16867).

**Abstract:** LLMs are transforming society and permeating into diverse applications.  As a result, they will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated games with each other, with human-like strategies, and actual human players. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioural signatures. In a large set of 2x2 games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behaviour of the simple convention to alternate between options. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice, a strategy we term social chain-of-thought (SCoT). Finally, we let different versions of GPT-4 play with human players and find that SCoT-prompting leads to better scores and more successful coordination between players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.
